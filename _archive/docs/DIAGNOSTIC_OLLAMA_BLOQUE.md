# üî¥ DIAGNOSTIC : Ollama Bloqu√© / Non R√©actif

## üîç Probl√®me Identifi√©

**Sympt√¥mes** :
- ‚ùå Ollama timeout apr√®s 30 secondes
- ‚ùå Aucune r√©ponse (ni phi3:mini, ni monchai-help)
- ‚ùå Mode d√©grad√© activ√© syst√©matiquement
- ‚úÖ Ollama API r√©pond (curl /api/tags fonctionne)
- ‚úÖ Mod√®les pr√©sents (ollama list fonctionne)

**Diagnostic** : Ollama est **d√©marr√©** mais **bloqu√©** lors de la g√©n√©ration.

---

## üéØ Causes Possibles

### 1. **Ollama Surcharg√© / Bloqu√©** (Probable)
- Processus Ollama fig√©
- Mod√®le charg√© en m√©moire mais non r√©actif
- GPU/CPU surcharg√©

### 2. **Mod√®le Corrompu** (Possible)
- Mod√®le mal t√©l√©charg√©
- Cache corrompu

### 3. **M√©moire Insuffisante** (Possible)
- RAM satur√©e
- Swap excessif

### 4. **Conflit de Processus** (Possible)
- Plusieurs instances Ollama
- Conflit de ports

---

## üîß Solutions

### Solution 1 : Red√©marrer Ollama (Recommand√©)

#### Windows

```powershell
# Arr√™ter Ollama
taskkill /F /IM ollama.exe

# Attendre 5 secondes
Start-Sleep -Seconds 5

# Red√©marrer Ollama
Start-Process ollama serve
```

#### Ou via Services Windows

1. Ouvrir "Services" (services.msc)
2. Chercher "Ollama"
3. Clic droit ‚Üí Red√©marrer

---

### Solution 2 : Vider le Cache Ollama

```bash
# Supprimer le cache
ollama rm phi3:mini
ollama rm monchai-help

# Re-t√©l√©charger
ollama pull phi3:mini
```

---

### Solution 3 : V√©rifier la M√©moire

```powershell
# V√©rifier la RAM disponible
Get-CimInstance Win32_OperatingSystem | Select-Object FreePhysicalMemory, TotalVisibleMemorySize

# V√©rifier les processus Ollama
Get-Process ollama
```

**Action** : Si RAM < 2 GB libre, fermer d'autres applications

---

### Solution 4 : Utiliser un Mod√®le Plus L√©ger

Si phi3:mini (2.2 GB) est trop lourd :

```bash
# T√©l√©charger gemma3:1b (815 MB)
ollama pull gemma3:1b
```

Puis modifier `.env` :
```env
HELP_MODEL=gemma3:1b
```

---

### Solution 5 : Mode D√©grad√© Permanent (Fallback)

Si Ollama ne fonctionne vraiment pas, d√©sactiver l'IA :

```env
# Dans .env
HELP_MODEL=none
```

Puis modifier `apps/ai/views.py` pour toujours utiliser le fallback :

```python
def help_query(request):
    # ... code existant ...
    
    # Forcer le mode d√©grad√©
    text = degraded_answer()
    resp = {
        'text': text,
        'page_effective': page_effective,
        'degraded': True,
    }
    return JsonResponse(resp, status=200)
```

---

## üß™ Tests de V√©rification

### Test 1 : Ollama r√©pond-il ?

```bash
curl http://localhost:11434/api/tags
```

**R√©sultat attendu** : Liste des mod√®les en < 1s

---

### Test 2 : G√©n√©ration simple

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:mini",
  "prompt": "Bonjour",
  "stream": false
}'
```

**R√©sultat attendu** : R√©ponse en < 5s

---

### Test 3 : Test Python direct

```bash
python test_ollama_direct.py
```

**R√©sultat attendu** : R√©ponse en < 5s

---

### Test 4 : Test complet

```bash
python test_help_performance.py
```

**R√©sultat attendu** : 
- Temps moyen < 5s
- Mode d√©grad√© : 0/3

---

## üìã Proc√©dure de R√©solution Compl√®te

### √âtape 1 : Diagnostic

```powershell
# V√©rifier si Ollama tourne
Get-Process ollama

# V√©rifier la RAM
Get-CimInstance Win32_OperatingSystem | Select FreePhysicalMemory

# Tester l'API
curl http://localhost:11434/api/tags
```

---

### √âtape 2 : Red√©marrage

```powershell
# Arr√™ter Ollama
taskkill /F /IM ollama.exe

# Attendre
Start-Sleep -Seconds 5

# Red√©marrer
Start-Process ollama serve

# Attendre le d√©marrage
Start-Sleep -Seconds 10
```

---

### √âtape 3 : Test Simple

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:mini",
  "prompt": "Test",
  "stream": false
}'
```

**Si √ßa marche** : Passer √† l'√©tape 4
**Si √ßa ne marche pas** : Passer √† l'√©tape 5

---

### √âtape 4 : Test Complet

```bash
python test_help_performance.py
```

**Si √ßa marche** : ‚úÖ Probl√®me r√©solu !
**Si √ßa ne marche pas** : Passer √† l'√©tape 5

---

### √âtape 5 : Mod√®le Plus L√©ger

```bash
# T√©l√©charger gemma3:1b
ollama pull gemma3:1b
```

Modifier `.env` :
```env
HELP_MODEL=gemma3:1b
HELP_TIMEOUT=5
```

Retester :
```bash
python test_help_performance.py
```

---

### √âtape 6 : Mode D√©grad√© (Dernier Recours)

Si rien ne fonctionne, d√©sactiver Ollama temporairement.

Cr√©er un fichier `apps/ai/views_fallback.py` :

```python
from django.http import JsonResponse

def help_query_fallback(request):
    """Version fallback sans Ollama."""
    import json
    data = json.loads(request.body.decode('utf-8') or '{}')
    page_url = data.get('page_url', '/')
    question = data.get('question', '')
    
    # R√©ponse g√©n√©rique
    text = f"""Aide rapide
    
Pour {question}, voici la proc√©dure g√©n√©rale :

1) Identifier le module concern√©
2) Ouvrir le module depuis le menu
3) Cr√©er ou chercher l'√©l√©ment
4) Compl√©ter les informations
5) Enregistrer

Pour une aide plus d√©taill√©e, consultez la documentation ou contactez le support.
"""
    
    return JsonResponse({
        'text': text,
        'page_effective': page_url,
        'degraded': True,
    })
```

Modifier `apps/ai/urls.py` :
```python
from .views_fallback import help_query_fallback

urlpatterns = [
    path('help/query', help_query_fallback, name='help_query'),  # Utiliser fallback
    # ...
]
```

---

## üéØ R√©sum√© des Actions

### Actions Imm√©diates

1. ‚úÖ **Red√©marrer Ollama**
   ```powershell
   taskkill /F /IM ollama.exe
   Start-Sleep -Seconds 5
   Start-Process ollama serve
   ```

2. ‚úÖ **Tester**
   ```bash
   python test_ollama_direct.py
   ```

3. ‚úÖ **Si √ßa ne marche pas : Mod√®le plus l√©ger**
   ```bash
   ollama pull gemma3:1b
   ```
   ```env
   HELP_MODEL=gemma3:1b
   ```

---

### Actions de Secours

4. ‚ö†Ô∏è **Si toujours bloqu√© : Vider le cache**
   ```bash
   ollama rm phi3:mini
   ollama pull phi3:mini
   ```

5. ‚ö†Ô∏è **Si vraiment rien ne marche : Mode d√©grad√©**
   - Utiliser `views_fallback.py`
   - D√©sactiver Ollama temporairement

---

## üìä Checklist de R√©solution

- [ ] Ollama red√©marr√©
- [ ] Test simple r√©ussi (curl)
- [ ] Test Python r√©ussi
- [ ] Test complet r√©ussi
- [ ] Temps de r√©ponse < 5s
- [ ] Mode d√©grad√© : 0/3
- [ ] UX acceptable

---

## üí° Recommandations Finales

### Court Terme
1. Red√©marrer Ollama
2. Utiliser gemma3:1b (plus l√©ger)
3. Augmenter le cache (3600s)

### Moyen Terme
1. Monitorer la RAM
2. Red√©marrer Ollama quotidiennement
3. Utiliser un mod√®le stable

### Long Terme
1. Envisager un serveur d√©di√© Ollama
2. Utiliser une API externe (OpenAI, Anthropic)
3. Impl√©menter un syst√®me de fallback robuste

---

*Diagnostic cr√©√© le : 29/10/2024*
*Probl√®me : Ollama bloqu√© / timeout 30s*
*Solution imm√©diate : Red√©marrer Ollama*
*Solution alternative : Mod√®le plus l√©ger (gemma3:1b)*
